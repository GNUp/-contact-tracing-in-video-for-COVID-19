# USAGE
# To read and write back out to video:
# python people_counter.py --prototxt mobilenet_ssd/MobileNetSSD_deploy.prototxt \
#       --model mobilenet_ssd/MobileNetSSD_deploy.caffemodel --input videos/example_01.mp4 \
#       --output output/output_01.avi
#
# To read from webcam and write back out to disk:
# python people_counter.py --prototxt mobilenet_ssd/MobileNetSSD_deploy.prototxt \
#       --model mobilenet_ssd/MobileNetSSD_deploy.caffemodel \
#       --output output/webcam_output.avi

# import the necessary packages
from openvino.inference_engine import IENetwork
from openvino.inference_engine import IEPlugin
from centroidtracker import CentroidTracker
from trackableobject import TrackableObject
from intel.yoloparams import TinyYOLOV3Params
from intel.tinyyolo import TinyYOLOv3
from imutils.video import FPS
from imutils.object_detection import non_max_suppression
from scipy.spatial import distance as disFuc
import numpy as np

import imutils
import os
import time
import dlib
import cv2
import bcc
import group
import time
import itertools
from conf import Conf

class Recorder:
    def __init__(self, argConf, argSkip, argPd, argMd, argTime, argPro, argModel, argConfiguration):
        self.argConf = argConf
        self.argSkip = argSkip
        self.argPd = argPd
        self.argMd = argMd
        self.argTime = argTime
        
        # load the configuration file
        self.conf = Conf(argConfiguration)
        
        # load the COCO class labels our YOLO model was trained on 
        self.LABELS = open(self.conf["labels_path"]).read().strip().split("\n")
        
        # initialize the plugin in for specified device
        self.plugin = IEPlugin(device="MYRIAD")
        
        # read the IR generated by the Model Optimizer (.xml and .bin files)
        print("[INFO] loading models...")
        self.net = IENetwork(model=self.conf["xml_path"], weights=self.conf["bin_path"])
        
        # prepare inputs
        print("[INFO] preparing inputs...")
        self.inputBlob = next(iter(self.net.inputs))
        
        # loading model to the plugin
        print("[INFO] loading model to the plugin...")
        self.execNet = self.plugin.load(network=self.net, num_requests=1)
        
        # set the default batch size as 1 and get the number of input blobs,
        # number of channels, the height, and width of the input blob
        self.net.batch_size = 1
        (self.n, self.c, self.h, self.w) = self.net.inputs[self.inputBlob].shape

        # initialize the frame dimensions (we'll set them as soon as we read
        # the first frame from the video)
        self.W = None
        self.H = None

        # instantiate our centroid tracker, then initialize a list to store
        # each of our dlib correlation trackers, followed by a dictionary to
        # map each unique object ID to a TrackableObject
        self.ct = CentroidTracker(maxDisappeared=10, maxDistance=50)
        self.trackers = []
        self.trackableObjects = {}

        # initialize the total number of frames processed thus far, along
        # with the total number of objects that have moved either up or down
        self.totalFrames = 0
        self.totalDown = 0
        self.totalUp = 0

        # start the frames per second throughput estimator
        self.fps = FPS().start()

        # initialize group list for tracking
        self.groupList = []
    
    # loop over frames from the video stream
    def forward(self, frame, depth):
        # resize original frame to have a maximum width of 500 pixel and
        # input_frame to network size
        frame = imutils.resize(frame, width=500)
        inputFrame = cv2.resize(frame, (self.w, self.h))
        
        # resize the frame to have a maximum width of 500 pixels (the
        # less data we have, the faster we can process it), then convert
        # the frame from BGR to RGB for dlib
        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        
        # change data layout from HxWxC to CxHxW
        inputFrame = inputFrame.transpose((2, 0, 1))
        inputFrame = inputFrame.reshape((self.n, self.c, self.h, self.w))

        # if the frame dimensions are empty, set them
        if self.W is None or self.H is None:
            (self.H, self.W) = frame.shape[:2]

        # initialize the current status along with our list of bounding
        # box rectangles returned by either (1) our object detector or
        # (2) the correlation trackers
        status = "Waiting"
        rects = []

        # check to see if we should run a more computationally expensive
        # object detection method to aid our tracker
        if self.totalFrames % self.argSkip == 0:
            # set the status and initialize our new set of object trackers
            status = "Detecting"
            self.trackers = []

            # start inference and initialize list to collect object detection
            # results
            output = self.execNet.infer({self.inputBlob: inputFrame})
            objects = []
            
            # loop over the output items
            for (layerName, outBlob) in output.items():
                # create a new object which contains the required tinyYOLOv3
                # parameters
                layerParams = TinyYOLOV3Params(self.net.layers[layerName].params,
                                               outBlob.shape[2])

                # parse the output region
                objects += TinyYOLOv3.parse_yolo_region(outBlob,
                inputFrame.shape[2:], frame.shape[:-1], layerParams,
                                                        self.conf["prob_threshold"])
            
            # loop over each of the objects
            for i in range(len(objects)):
                # check if the confidence of the detected object is zero, if
                # it is, then skip this iteration, indicating that the object
                # should be ignored
                if objects[i]["confidence"] == 0:
                    continue

                # loop over remaining objects
                for j in range(i + 1, len(objects)):
                    # check if the IoU of both the objects exceeds a
                    # threshold, if it does, then set the confidence of that
                    # object to zero
                    if TinyYOLOv3.intersection_over_union(objects[i],
                                                          objects[j]) > self.conf["iou_threshold"]:
                        objects[j]["confidence"] = 0
            
            # filter objects by using the probability threshold -- if a an
            # object is below the threshold, ignore it
            objects = [obj for obj in objects if obj['confidence'] >= self.conf["prob_threshold"]]
            
            # store the height and width of the original frame
            (endY, endX) = frame.shape[:-1]
            
            # loop through all the remaining objects
            for obj in objects:
                # validate the bounding box of the detected object, ensuring
                # we don't have any invalid bounding boxes
                if obj["xmax"] > endX or obj["ymax"] > endY or obj["xmin"] < 0 or obj["ymin"] < 0:
                    continue
                
                label = self.LABELS[obj["class_id"]]
                if label == "person":
                
                    tracker = dlib.correlation_tracker()
                    rect = dlib.rectangle(obj["xmin"], obj["ymin"], obj["xmax"], obj["ymax"])
                    tracker.start_track(rgb, rect)
                
                    self.trackers.append(tracker)

        # otherwise, we should utilize our object *trackers* rather than
        # object *detectors* to obtain a higher frame processing throughput
        # set the status of our system to be 'tracking' rather
        # than 'waiting' or 'detecting'
        status = "Tracking"
        # loop over the trackers
        for tracker in self.trackers:
            # update the tracker and grab the updated position
            tracker.update(rgb)
            pos = tracker.get_position()
            # unpack the position object
            startX = int(pos.left())
            startY = int(pos.top())
            endX = int(pos.right())
            endY = int(pos.bottom())
            # add the bounding box coordinates to the rectangles list
            rects.append((startX, startY, endX, endY))

        # use the centroid tracker to associate the (1) old object
        # centroids with (2) the newly computed object centroids
        objects = self.ct.update(rects)

        # record close contact of people in the frame
        if self.totalFrames % self.argSkip == 5:
            objectList = list(
                map(lambda x: (x[0], x[1][0]), list(objects.items())))
            g = bcc.Graph(len(objectList))

            def discoverEdge(obList, g):
                if len(obList) == 0:
                    return
                else:
                    originVertex = obList[0]
                    destinationVertices = obList[1:]
                    for destinationVertex in destinationVertices:
                        (oriObjectID, oriCentroid) = originVertex
                        (desObjectID, desCentroid) = destinationVertex

                        distanceBtwObjs = disFuc.euclidean(
                            oriCentroid, desCentroid)
                        if 0 < oriCentroid[0] < 640 and 0 < oriCentroid[1] < 480 \
                            and 0 < desCentroid[0] < 640 and 0 < desCentroid[1] < 480:
                            oriDistanceFromCam = depth.get_distance(
                                oriCentroid[0], oriCentroid[1])
                            desDistanceFromCam = depth.get_distance(
                                desCentroid[0], desCentroid[1])
                        else: # fail to get a depth from the pixel
                            oriDistanceFromCam = 0
                            desDistanceFromCam = 9999999999999

                        if distanceBtwObjs < self.argPd and abs(oriDistanceFromCam - desDistanceFromCam) < self.argMd:
                            g.addEdge(oriObjectID, desObjectID)
                    discoverEdge(destinationVertices, g)
            discoverEdge(objectList, g)

            # get update group list for tracking their remaining time
            newGroupList = g.BCC()
            self.groupList = group.updateGroupList(self.groupList, newGroupList)

            # capture long-lasting group and renew group list
            capturedGroupList = []
            for g in self.groupList:
                if abs(time.time() - g.timestamp) > self.argTime and not g.captured and len(g.idGroup) != 1:
                    capturedRects = []
                    for idx in g.idGroup:
                        (centroid, rect) = objects[idx]
                        capturedRects.append(rect)

                    (startX, startY, endX, endY) = merge_recs(capturedRects)[0]
                    cropImg = frame[startY:endY, startX:endX]
                    timestr = time.strftime("%Y%m%d-%H%M%S")
                    cv2.imwrite("capture/" + timestr + ".png", cropImg)
                    print(
                        "-----------------------------Capured!-----------------------------")

                    g.captured = True
                    capturedGroupList.append(g)
                else:
                    capturedGroupList.append(g)
                    
            self.groupList = capturedGroupList
                
        # draw line between members of a detected group
        for g in self.groupList:
            if len(g.idGroup) != 1:
                for (oriID, desID) in itertools.combinations(g.idGroup, 2):
                    try:
                        oriCentroid = tuple(objects[oriID][0])
                        desCentroid = tuple(objects[desID][0])
                        cv2.line(frame, oriCentroid, desCentroid, (0, 0, 255), 2)
                    except:
                        pass

        # loop over the tracked objects
        for (objectID, (centroid, rect)) in objects.items():
                # check to see if a trackable object exists for the current
                # object ID
            to = self.trackableObjects.get(objectID, None)

            # if there is no existing trackable object, create one
            if to is None:
                to = TrackableObject(objectID, centroid)

            # store the trackable object in our dictionary
            self.trackableObjects[objectID] = to

            # draw both the ID of the object, the distance of the object,
            # and the centroid of the object on the output frame
            if 0 < centroid[0] < 640 and 0 < centroid[1] < 480:
                distance = depth.get_distance(centroid[0], centroid[1])
            else:
                distance = 0
            text = "Person {} - {}m".format(objectID, round(distance, 2))
            (startX, startY, endX, endY) = rect
            y = startY - 10 if startY - 10 > 10 else startY + 10
            cv2.putText(frame, text, (startX, y),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

            # draw the green bounding box if the object is not included in any group,
            # otherwise red bounding box
            if isGrouped(self.groupList, objectID):
                cv2.rectangle(frame, (startX, startY), (endX, endY), (0, 0, 255), 2)
                cv2.circle(frame, (centroid[0], centroid[1]), 4, (0, 0, 255), -1)
            else:
                cv2.rectangle(frame, (startX, startY), (endX, endY), (0, 255, 0), 2)

        # cv2.imwrite("capture/full.png", frame)

        # construct a tuple of information we will be displaying on the
        # frame
        info = [
            ("Social Distancing Violations", len(list(filter(lambda x: len(x.idGroup) != 1, self.groupList))))
        ]

        # loop over the info tuples and draw them on our frame
        for (i, (k, v)) in enumerate(info):
            text = "{}: {}".format(k, v)
            cv2.putText(frame, text, (10, self.H - ((i * 20) + 20)),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)
    
        # increment the total number of frames processed thus far and
        # then update the FPS counter
        self.totalFrames += 1
        
        return frame

def merge_recs(rects):
    result = (640, 480, 0, 0)
    for rect in rects:
        result = union(result, rect)
    return [result]


def union(a, b):
    x = min(a[0], b[0])
    y = min(a[1], b[1])
    w = max(a[2], b[2])
    h = max(a[3], b[3])
    return (x, y, w, h)

def isGrouped(groupList, objID):
    for g in groupList:
        if objID in g.idGroup and len(g.idGroup) > 1:
             return True
    
    return False
